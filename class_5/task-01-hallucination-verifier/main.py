#!/usr/bin/env python3
"""
Main demonstration script for Task 05 - Hallucination Verifier
"""

import os
import json
from solution import verify_hallucination, evaluate_hallucination_detection, llm_as_judge_evaluation

def main():
    print("üîç Hallucination Verifier Demo")
    print("=" * 50)
    
    # Demo questions
    demo_questions = [
        "Who was the first person to walk on Mars?",
        "What is the capital of France?",
        "Who invented the computer mouse?",
        "What is the population of the planet Jupiter?"
    ]
    
    for i, question in enumerate(demo_questions, 1):
        print(f"\nüìù Question {i}: {question}")
        print("-" * 40)
        
        result = verify_hallucination(question)
        
        print(f"üí¨ Answer: {result['answer']}")
        print(f"üìö Sources: {result['sources']}")
        print(f"üéØ Is Hallucination: {result['is_hallucination']}")
        print(f"üìä Confidence: {result['confidence']:.2f}")
        print(f"üß† Reasoning: {result['reasoning']}")
        
    print("\n" + "=" * 50)
    print("Demo completed! ‚úÖ")

def evaluation_demo():
    """Demonstrate comprehensive evaluation capabilities."""
    print("\nüß™ Evaluation Demo")
    print("=" * 50)
    
    # Load test questions from sample data
    test_questions = []
    try:
        with open('sample_questions.jsonl', 'r') as f:
            for line in f:
                test_questions.append(json.loads(line.strip()))
    except FileNotFoundError:
        # Fallback test questions
        test_questions = [
            {"question": "Who was the first person to walk on Mars?", "expected_hallucination": True},
            {"question": "What is the capital of France?", "expected_hallucination": False},
            {"question": "Who invented the computer mouse?", "expected_hallucination": False},
            {"question": "What is the population of the planet Jupiter?", "expected_hallucination": True}
        ]
    
    print(f"üìä Running evaluation on {len(test_questions)} questions...")
    
    # Basic accuracy evaluation
    print("\n1Ô∏è‚É£ Basic Accuracy Evaluation")
    print("-" * 30)
    basic_results = evaluate_hallucination_detection(test_questions)
    print(f"‚úÖ Accuracy: {basic_results['accuracy']:.2%}")
    print(f"üìà Correct: {basic_results['correct']}/{basic_results['total']}")
    
    # LLM-as-Judge comprehensive evaluation
    print("\n2Ô∏è‚É£ LLM-as-Judge Comprehensive Evaluation")
    print("-" * 30)
    print("ü§ñ Running comprehensive analysis...")
    
    llm_results = llm_as_judge_evaluation(test_questions)
    
    print(f"\nüèÜ Performance Level: {llm_results['performance_level']}")
    print(f"üéØ Binary Accuracy: {llm_results['binary_accuracy']:.2%}")
    print(f"üìä Average Scores (0-10 scale):")
    print(f"   ‚Ä¢ Output Accuracy: {llm_results['avg_output_accuracy']:.1f}")
    print(f"   ‚Ä¢ Reasoning Quality: {llm_results['avg_reasoning_quality']:.1f}")
    print(f"   ‚Ä¢ Source Quality: {llm_results['avg_source_quality']:.1f}")
    print(f"   ‚Ä¢ Process Methodology: {llm_results['avg_process_methodology']:.1f}")
    print(f"   ‚Ä¢ Confidence Calibration: {llm_results['avg_confidence_calibration']:.1f}")
    print(f"   ‚Ä¢ Overall Assessment: {llm_results['avg_overall_assessment']:.1f}")
    
    print("\nüí° Evaluation Insights:")
    if llm_results['avg_overall_assessment'] >= 8.0:
        print("   üåü Excellent performance! The system demonstrates superior hallucination detection.")
    elif llm_results['avg_overall_assessment'] >= 6.5:
        print("   üëç Good performance with room for fine-tuning prompting techniques.")
    elif llm_results['avg_overall_assessment'] >= 5.0:
        print("   ‚ö†Ô∏è  Adequate performance but consider improving source verification.")
    else:
        print("   üîß Needs improvement in methodology and prompt engineering.")

if __name__ == "__main__":
    # Check for required API key
    if not os.getenv("OPENAI_API_KEY"):
        print("‚ùå Error: OPENAI_API_KEY environment variable not set")
        print("Please set your OpenAI API key and try again.")
        exit(1)
    
    # Run main demo
    main()
    
    # Run evaluation demo
    evaluation_demo() 